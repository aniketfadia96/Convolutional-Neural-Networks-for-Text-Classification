{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8grJ2sHDHrZ"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4791,
     "status": "ok",
     "timestamp": 1592314286118,
     "user": {
      "displayName": "Aniket Fadia",
      "photoUrl": "",
      "userId": "02858654771955807579"
     },
     "user_tz": -330
    },
    "id": "AMLCg876CrLr",
    "outputId": "99f97dcf-4792-44bf-b95a-2470136745b5"
   },
   "outputs": [],
   "source": [
    "# Python\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sys import getsizeof\n",
    "import itertools\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TQDM\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "\n",
    "# sklearn\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keras\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "# Keras Pre-processing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Keras Layers\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, MaxPooling1D, Dropout, Concatenate, Flatten, Add\n",
    "\n",
    "# Keras Optimizer\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "# Keras Model Saving\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23_Wu1-qFXNE"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1877,
     "status": "ok",
     "timestamp": 1592314292206,
     "user": {
      "displayName": "Aniket Fadia",
      "photoUrl": "",
      "userId": "02858654771955807579"
     },
     "user_tz": -330
    },
    "id": "kmKr7e5CGLAI",
    "outputId": "8575b367-26de-46d8-bd72-074a2dd7d4dd"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3979,
     "status": "ok",
     "timestamp": 1592314305439,
     "user": {
      "displayName": "Aniket Fadia",
      "photoUrl": "",
      "userId": "02858654771955807579"
     },
     "user_tz": -330
    },
    "id": "_SqsTXUxNv1H",
    "outputId": "f9aaa65d-45b1-4661-faa5-c04ad6dc17ad"
   },
   "outputs": [],
   "source": [
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pekpxXmACrMa"
   },
   "outputs": [],
   "source": [
    "# Add path to your project directory\n",
    "PROJECT_DIR = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvSkxdkUG_q1"
   },
   "outputs": [],
   "source": [
    "# Add the path to your data directory\n",
    "DATA_DIR = PROJECT_DIR + 'data/aclImdb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M113ma8bODar"
   },
   "outputs": [],
   "source": [
    "# Add the path to your model directory\n",
    "MODEL_DIR = PROJECT_DIR + 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FSTiz8S6Pot0"
   },
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "    corpus = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(directory + filename, 'r', encoding='latin1') as f:\n",
    "            movie_review = f.read()\n",
    "            movie_review = movie_review.replace('<br />', '')\n",
    "            to_tokenize = '!\\\"#$%&\\'()*+, -./:;<=>?@[\\]^_`{|}~'\n",
    "            # Add space before punctuations\n",
    "            movie_review = re.sub(r'(['+to_tokenize+'])', r' \\1 ', movie_review)\n",
    "            corpus.append(movie_review)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4W-_HIzGJB6b"
   },
   "source": [
    "## Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5dJZBgCgRhu_"
   },
   "outputs": [],
   "source": [
    "# Load Positive Movie Reviews\n",
    "positive_train_data = load_data(DATA_DIR + 'train/pos/')\n",
    "print(\"Number of positive movie reviews in train data: {}\".format(len(positive_train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1PJcnBBRmvY"
   },
   "outputs": [],
   "source": [
    "# Load Negative Movie Reviews\n",
    "negative_train_data = load_data(DATA_DIR + 'train/neg/')\n",
    "print(\"Number of negative movie reviews in train data: {}\".format(len(negative_train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RodMbvE_RmWi"
   },
   "outputs": [],
   "source": [
    "# Combine into train data\n",
    "train_data = positive_train_data + negative_train_data\n",
    "\n",
    "train_labels = np.zeros(25000)\n",
    "train_labels[0:len(positive_train_data) - 1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DoJjVlFjLA7f"
   },
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOWCvgQOK2y4"
   },
   "outputs": [],
   "source": [
    "# Load Positive Movie Reviews\n",
    "positive_test_data = load_data(DATA_DIR + 'test/pos/')\n",
    "print(\"Number of positive movie reviews in test data: {}\".format(len(positive_test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ugkCMzIVLK0C"
   },
   "outputs": [],
   "source": [
    "# Load Negative Movie Reviews\n",
    "negative_test_data = load_data(DATA_DIR + 'train/neg/')\n",
    "print(\"Number of negative movie reviews in test data: {}\".format(len(negative_test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t5DXnGNwK54a"
   },
   "outputs": [],
   "source": [
    "# Combine into test data\n",
    "test_data = positive_test_data + negative_test_data\n",
    "\n",
    "test_labels = np.zeros(25000)\n",
    "test_labels[0:len(positive_test_data) - 1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kb-sogj7iJZY"
   },
   "source": [
    "## Test-Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XHFDml4GiPcd"
   },
   "outputs": [],
   "source": [
    "X_test, X_val, Y_test, Y_val = train_test_split(test_corpus, test_label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sqBCgEZDLQwv"
   },
   "source": [
    "# Preprocess "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vaYRYq3dLUBn"
   },
   "source": [
    "## Max Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzTrWCThK6Nl"
   },
   "outputs": [],
   "source": [
    "def find_max_seq_len(corpus):\n",
    "    corpus_len = []\n",
    "    for review in corpus:\n",
    "        review_list = (str(review)).split()\n",
    "        corpus_len.append(len(review_list)\n",
    "  \n",
    "  return max(corpus_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1305,
     "status": "ok",
     "timestamp": 1591703633031,
     "user": {
      "displayName": "Aniket Fadia",
      "photoUrl": "",
      "userId": "02858654771955807579"
     },
     "user_tz": -330
    },
    "id": "aCD4VetZK7E8",
    "outputId": "654e7b79-40e1-4fc4-930a-5f957799ccb1"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = max(find_max_seq_len(train_data),\n",
    "                          find_max_seq_len(test_data))\n",
    "\n",
    "print(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HnjjodD5NyFG"
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76ZXaW5EK7_q"
   },
   "outputs": [],
   "source": [
    "# Keep all the punctuations  -- Since, GloVe has embeddings for them\n",
    "tokenizer = Tokenizer(filters='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10729,
     "status": "ok",
     "timestamp": 1591703669070,
     "user": {
      "displayName": "Aniket Fadia",
      "photoUrl": "",
      "userId": "02858654771955807579"
     },
     "user_tz": -330
    },
    "id": "vm9YbrnnQnxa",
    "outputId": "f2e5f026-78ac-45c7-cc03-c5855eb6873c"
   },
   "outputs": [],
   "source": [
    "# Fit the tokenizer on entire train corpus\n",
    "tokenizer.fit_on_texts(train_corpus)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found %s unique tokens: \" % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMfq6nkLWDE3"
   },
   "source": [
    "# Generate Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNLRtbHUWAP9"
   },
   "outputs": [],
   "source": [
    "def generate_indices(train_corpus, val_corpus, test_corpus, max_sequence_len):\n",
    "  \n",
    "    train_sequences = tokenizer.texts_to_sequences(train_corpus)\n",
    "    train_indices = pad_sequences(train_sequences, maxlen=max_sequence_len)\n",
    "\n",
    "    val_sequences = tokenizer.texts_to_sequences(val_corpus)\n",
    "    val_indices = pad_sequences(val_sequences, maxlen=max_sequence_len)\n",
    "\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_corpus)\n",
    "    test_indices = pad_sequences(test_sequences, maxlen=max_sequence_len)\n",
    "\n",
    "    sequences2 = tokenizer.texts_to_sequences(question2)\n",
    "    indices2 = pad_sequences(sequences2, maxlen=max_sequence_len)\n",
    "\n",
    "    return train_indices, val_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAF3Pt96j119"
   },
   "outputs": [],
   "source": [
    "train_indices, val_indices, test_indices = generate_indices(train_corpus, X_val, X_test, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XRLYrlhaGe2"
   },
   "source": [
    "## Reverse Map the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uM84_MfxaC4z"
   },
   "outputs": [],
   "source": [
    "# Create a reverse dictionary \n",
    "reverse_word_map = {v : k for k, v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6RvqAFsaC1Z"
   },
   "outputs": [],
   "source": [
    "def indices_to_text(indices_list):\n",
    "    text = []\n",
    "    for i in indices_list:\n",
    "        if i is not 0:\n",
    "            text.append(reverse_word_map.get(i))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 723,
     "status": "ok",
     "timestamp": 1591703782098,
     "user": {
      "displayName": "Aniket Fadia",
      "photoUrl": "",
      "userId": "02858654771955807579"
     },
     "user_tz": -330
    },
    "id": "f_rt5ujxaCyG",
    "outputId": "a1953118-6e9a-482c-d9d7-cdca0dc8b159"
   },
   "outputs": [],
   "source": [
    "print(train_corpus[0])\n",
    "print(train_indices[0])\n",
    "print(indices_to_text(train_indices[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w75Moqh5c6z7"
   },
   "source": [
    "# Prepare embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFnfUVxRc_ia"
   },
   "source": [
    "## Load the GloVe embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43322,
     "status": "ok",
     "timestamp": 1591703975950,
     "user": {
      "displayName": "Aniket Fadia",
      "photoUrl": "",
      "userId": "02858654771955807579"
     },
     "user_tz": -330
    },
    "id": "2fX48ryHciwS",
    "outputId": "4d18b2dd-0813-443f-d4f7-b7580c5942e5"
   },
   "outputs": [],
   "source": [
    "# Add the path to your GloVe embedding vector directory\n",
    "GLOVE_DIR = ''\n",
    "\n",
    "glove_embedding_index = {}\n",
    "\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove_embedding_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(\"Found %s word vectors\" % len(glove_embedding_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOzV5Mj5eI_0"
   },
   "source": [
    "## Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1053,
     "status": "ok",
     "timestamp": 1591704023755,
     "user": {
      "displayName": "Aniket Fadia",
      "photoUrl": "",
      "userId": "02858654771955807579"
     },
     "user_tz": -330
    },
    "id": "Oxkymwx8eCr5",
    "outputId": "1b7fe552-6c03-472d-ec07-a86742ff78da"
   },
   "outputs": [],
   "source": [
    "# Vocabulary Size\n",
    "VOCAB_SIZE = len(word_index)\n",
    "print(\"Vocabulary size is: {}\".format(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hodtlVPeDli"
   },
   "outputs": [],
   "source": [
    "# Randomly initialize the embedding matrix for unknown words\n",
    "embedding_matrix = np.random.rand(VOCAB_SIZE + 1, 300)\n",
    "\n",
    "# Padding will be ignored\n",
    "embedding_matrix[0] = 0 \n",
    "\n",
    "for word, i in word_index.items():\n",
    "    glove_embedding_vector = glove_embedding_index.get(word)\n",
    "    if glove_embedding_vector is not None:\n",
    "        embedding_matrix[i] = glove_embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EgaWgflxpvmS"
   },
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qWRTyC1tFwyv"
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mo1j1Pxbsrjr"
   },
   "source": [
    "## Random Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m99VJB7QeFqs"
   },
   "outputs": [],
   "source": [
    "def train_random_model():\n",
    "  \n",
    "    # Input Layer\n",
    "    input_vec = Input(shape=(MAX_SEQUENCE_LENGTH,), name='Input_Layer', dtype='int32')\n",
    "\n",
    "    # Embedding Layer\n",
    "    embeddings_random = np.random.rand(VOCAB_SIZE+1, 300)\n",
    "    embedding_layer = Embedding(input_dim=VOCAB_SIZE+1, output_dim=300, input_length = MAX_SEQUENCE_LENGTH, weights=[embeddings_random], trainable=True)\n",
    "\n",
    "    # Embedded version of the inputs\n",
    "    embedding_out = embedding_layer(input_vec)\n",
    "\n",
    "    kernel_size1 = 3\n",
    "    conv1 = Conv1D(filters=100, kernel_size=kernel_size1, activation='relu', kernel_constraint=max_norm(3))(embedding_out)\n",
    "    pool1 = MaxPooling1D(MAX_SEQUENCE_LENGTH - kernel_size1 + 1)(conv1)\n",
    "    out1 = Flatten()(pool1)\n",
    "\n",
    "    kernel_size2 = 4\n",
    "    conv2 = Conv1D(filters=100, kernel_size=kernel_size2, activation='relu', kernel_constraint=max_norm(3))(embedding_out)\n",
    "    pool2 = MaxPooling1D(MAX_SEQUENCE_LENGTH - kernel_size2 + 1)(conv2)\n",
    "    out2 = Flatten()(pool2)\n",
    "\n",
    "    kernel_size3 = 5\n",
    "    conv3 = Conv1D(filters=100, kernel_size=kernel_size3, activation='relu', kernel_constraint=max_norm(3))(embedding_out)\n",
    "    pool3 = MaxPooling1D(MAX_SEQUENCE_LENGTH - kernel_size3 + 1)(conv3)\n",
    "    out3 = Flatten()(pool3)\n",
    "\n",
    "    final_out = Concatenate()([out1, out2, out3])\n",
    "\n",
    "    final_out = Dropout(0.5)(final_out)\n",
    "\n",
    "    final_out = Dense(1, activation='sigmoid')(final_out)\n",
    "\n",
    "    # Pack it all up into a model\n",
    "    model = Model(inputs=input_vec, outputs=final_out)\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ohWYn5iunkb"
   },
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OZMCcbDeFbb"
   },
   "outputs": [],
   "source": [
    "random_model = train_randon_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1283,
     "status": "ok",
     "timestamp": 1591704209762,
     "user": {
      "displayName": "Aniket Fadia",
      "photoUrl": "",
      "userId": "02858654771955807579"
     },
     "user_tz": -330
    },
    "id": "miZ2AuPfeFWj",
    "outputId": "55761916-e177-425b-ad01-099282bc1e0f"
   },
   "outputs": [],
   "source": [
    "random_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ebi7uXJIu2xd"
   },
   "source": [
    "### Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Rt74OxCvQAn"
   },
   "outputs": [],
   "source": [
    "random_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9tPjYXg8u3Be"
   },
   "source": [
    "### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CE7QX3mcvk83"
   },
   "outputs": [],
   "source": [
    "# Create a checkpoint\n",
    "filepath = MODEL_DIR + 'random_model_best.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1420328,
     "status": "ok",
     "timestamp": 1591709657180,
     "user": {
      "displayName": "Aniket Fadia",
      "photoUrl": "",
      "userId": "02858654771955807579"
     },
     "user_tz": -330
    },
    "id": "6LJ9nvOMwvNw",
    "outputId": "af1589ed-c563-474f-e2ec-f404a005fc0f"
   },
   "outputs": [],
   "source": [
    "random_model_trained = random_model.fit(x=train_indices, y=train_labels,\n",
    "                                  batch_size=batch_size, nb_epoch=num_epochs, callbacks=callbacks_list, verbose=2,\n",
    "                                  validation_data=(val_indices, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5X8Y4Wfax1Af"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tD2qqrXjwvCn"
   },
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "random_model_json = random_model.to_json()\n",
    "\n",
    "with open(MODEL_DIR + 'random_model_json.json', 'w') as json_file:\n",
    "    json_file.write(random_model_json)\n",
    "\n",
    "print('Saved Random model architecture to the disk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qxyI_V24zGqc"
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBJCc2HHzJ3_"
   },
   "outputs": [],
   "source": [
    "# Load architecture\n",
    "with open(MODEL_DIR + 'random_model_json.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "\n",
    "random_model = model_from_json(model_json)\n",
    "\n",
    "# Load weights\n",
    "random_model.load_weights(MODEL_DIR + 'random_model_best.hdf5')\n",
    "print('Loaded Random model from the disk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_hfnWMTA61R"
   },
   "source": [
    "## Static Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "drTf_2lGBAGq"
   },
   "outputs": [],
   "source": [
    "def train_static_model(embedding_pretrained):\n",
    "  \n",
    "    # Input Layer\n",
    "    input_vec = Input(shape=(MAX_SEQUENCE_LENGTH,), name='Input_Layer', dtype='int32')\n",
    "\n",
    "    # Embedding Layer\n",
    "    embedding_layer = Embedding(input_dim=VOCAB_SIZE+1, output_dim=300, input_length = MAX_SEQUENCE_LENGTH, weights=[embedding_pretrained], trainable=False)\n",
    "\n",
    "    # Embedded version of the inputs\n",
    "    embedding_out = embedding_layer(input_vec)\n",
    "\n",
    "    kernel_size1 = 3\n",
    "    conv1 = Conv1D(filters=100, kernel_size=kernel_size1, activation='relu', kernel_constraint=max_norm(3))(embedding_out)\n",
    "    pool1 = MaxPooling1D(MAX_SEQUENCE_LENGTH - kernel_size1 + 1)(conv1)\n",
    "    out1 = Flatten()(pool1)\n",
    "\n",
    "    kernel_size2 = 4\n",
    "    conv2 = Conv1D(filters=100, kernel_size=kernel_size2, activation='relu', kernel_constraint=max_norm(3))(embedding_out)\n",
    "    pool2 = MaxPooling1D(MAX_SEQUENCE_LENGTH - kernel_size2 + 1)(conv2)\n",
    "    out2 = Flatten()(pool2)\n",
    "\n",
    "    kernel_size3 = 5\n",
    "    conv3 = Conv1D(filters=100, kernel_size=kernel_size3, activation='relu', kernel_constraint=max_norm(3))(embedding_out)\n",
    "    pool3 = MaxPooling1D(MAX_SEQUENCE_LENGTH - kernel_size3 + 1)(conv3)\n",
    "    out3 = Flatten()(pool3)\n",
    "\n",
    "    final_out = Concatenate()([out1, out2, out3])\n",
    "\n",
    "    final_out = Dropout(0.5)(final_out)\n",
    "\n",
    "    final_out = Dense(1, activation='sigmoid')(final_out)\n",
    "\n",
    "    # Pack it all up into a model\n",
    "    model = Model(inputs=input_vec, outputs=final_out)\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B7OFofXQB6aq"
   },
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aaIOoIq-B8gd"
   },
   "outputs": [],
   "source": [
    "static_model = train_static_model(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KpB5AxqzCgNG"
   },
   "outputs": [],
   "source": [
    "static_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzNNq9K_B9ru"
   },
   "source": [
    "### Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFkevGUqCBYG"
   },
   "outputs": [],
   "source": [
    "static_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-jhHOwPCCzQ"
   },
   "source": [
    "### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5V2Ip45zCEs3"
   },
   "outputs": [],
   "source": [
    "# Create a checkpoint\n",
    "filepath = MODEL_DIR + 'static_model_best.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ1z0WK4Cznl"
   },
   "outputs": [],
   "source": [
    "static_model_trained = static_model.fit(x=train_indices, y=train_labels,\n",
    "                                  batch_size=batch_size, nb_epoch=num_epochs, callbacks=callbacks_list, verbose=2,\n",
    "                                  validation_data=(val_indices, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SyM60-NdCFgs"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZDy0j_NCHPg"
   },
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "static_model_json = static_model.to_json()\n",
    "\n",
    "with open(MODEL_DIR + 'static_model_json.json', 'w') as json_file:\n",
    "    json_file.write(static_model_json)\n",
    "\n",
    "print('Saved Static model architecture to the disk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "augfGeR7CMJz"
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i3XkENKnCPBs"
   },
   "outputs": [],
   "source": [
    "# Load architecture\n",
    "with open(MODEL_DIR + 'static_model_json.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "\n",
    "static_model = model_from_json(model_json)\n",
    "\n",
    "# Load weights\n",
    "static_model.load_weights(MODEL_DIR + 'static_model_best.hdf5')\n",
    "print('Loaded Static model from the disk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uR17koqaDO_e"
   },
   "source": [
    "## Non-Static Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76WcYzsrDUQM"
   },
   "outputs": [],
   "source": [
    "def train_non_static_model(embedding_pretrained):\n",
    "  \n",
    "    # Input Layer\n",
    "    input_vec = Input(shape=(MAX_SEQUENCE_LENGTH,), name='Input_Layer', dtype='int32')\n",
    "\n",
    "    # Embedding Layer\n",
    "    embedding_layer = Embedding(input_dim=VOCAB_SIZE+1, output_dim=300, input_length = MAX_SEQUENCE_LENGTH, weights=[embedding_pretrained], trainable=True)\n",
    "\n",
    "    # Embedded version of the inputs\n",
    "    embedding_out = embedding_layer(input_vec)\n",
    "\n",
    "    kernel_size1 = 3\n",
    "    conv1 = Conv1D(filters=100, kernel_size=kernel_size1, activation='relu', kernel_constraint=max_norm(3))(embedding_out)\n",
    "    pool1 = MaxPooling1D(MAX_SEQUENCE_LENGTH - kernel_size1 + 1)(conv1)\n",
    "    out1 = Flatten()(pool1)\n",
    "\n",
    "    kernel_size2 = 4\n",
    "    conv2 = Conv1D(filters=100, kernel_size=kernel_size2, activation='relu', kernel_constraint=max_norm(3))(embedding_out)\n",
    "    pool2 = MaxPooling1D(MAX_SEQUENCE_LENGTH - kernel_size2 + 1)(conv2)\n",
    "    out2 = Flatten()(pool2)\n",
    "\n",
    "    kernel_size3 = 5\n",
    "    conv3 = Conv1D(filters=100, kernel_size=kernel_size3, activation='relu', kernel_constraint=max_norm(3))(embedding_out)\n",
    "    pool3 = MaxPooling1D(MAX_SEQUENCE_LENGTH - kernel_size3 + 1)(conv3)\n",
    "    out3 = Flatten()(pool3)\n",
    "\n",
    "    final_out = Concatenate()([out1, out2, out3])\n",
    "\n",
    "    final_out = Dropout(0.5)(final_out)\n",
    "\n",
    "    final_out = Dense(1, activation='sigmoid')(final_out)\n",
    "\n",
    "    # Pack it all up into a model\n",
    "    model = Model(inputs=input_vec, outputs=final_out)\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p0jUL0p4DVPD"
   },
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LZIzWtauDeBK"
   },
   "outputs": [],
   "source": [
    "non_static_model = train_non_static_model(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_E3n-fs0ENvr"
   },
   "outputs": [],
   "source": [
    "non_static_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ud-b4xCeDfkP"
   },
   "source": [
    "### Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0irRhmFDpr0"
   },
   "outputs": [],
   "source": [
    "non_static_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQnILprrDqgx"
   },
   "source": [
    "### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fk-LxcSJDs6i"
   },
   "outputs": [],
   "source": [
    "# Create a checkpoint\n",
    "filepath = MODEL_DIR + 'non_static_model_best.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QsTGfZWXEclx"
   },
   "outputs": [],
   "source": [
    "non_static_model_trained = non_static_model.fit(x=train_indices, y=train_labels,\n",
    "                                  batch_size=batch_size, nb_epoch=num_epochs, callbacks=callbacks_list, verbose=2,\n",
    "                                  validation_data=(val_indices, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2hdtbOjDxPU"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QYeQAPR9Dy_B"
   },
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "non_static_model_json = non_static_model.to_json()\n",
    "\n",
    "with open(MODEL_DIR + 'non_static_model_json.json', 'w') as json_file:\n",
    "    json_file.write(non_static_model_json)\n",
    "\n",
    "print('Saved Non Static model architecture to the disk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MR1In0DtD0UL"
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0QojJQkID2iN"
   },
   "outputs": [],
   "source": [
    "# Load architecture\n",
    "with open(MODEL_DIR + 'non_static_model_json.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "\n",
    "non_static_model = model_from_json(model_json)\n",
    "\n",
    "# Load weights\n",
    "non_static_model.load_weights(MODEL_DIR + 'non_static_model_best.hdf5')\n",
    "print('Loaded Non Static model from the disk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f1IL-nJ5EzU0"
   },
   "source": [
    "## Multi-Channel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c2RyPOm_E4mH"
   },
   "outputs": [],
   "source": [
    "def train_multichannel_model(embedding_pretrained):\n",
    "  \n",
    "    # Input Layer\n",
    "    input_vec = Input(shape=(MAX_SEQUENCE_LENGTH,), name='Input_Layer', dtype='int32')\n",
    "\n",
    "    # Embedding Layers\n",
    "    embedding_layer1 = Embedding(input_dim=VOCAB_SIZE+1, output_dim=300, input_length = MAX_SEQUENCE_LENGTH, weights=[embedding_pretrained], trainable=False)\n",
    "    embedding_layer2 = Embedding(input_dim=VOCAB_SIZE+1, output_dim=300, input_length = MAX_SEQUENCE_LENGTH, weights=[embedding_pretrained], trainable=True)\n",
    "\n",
    "    # Embedded version of the inputs\n",
    "    embedding_out1 = embedding_layer1(input_vec)\n",
    "    embedding_out2 = embedding_layer2(input_vec)\n",
    "\n",
    "    kernel_size1 = 3\n",
    "    conv1 = Conv1D(filters=100, kernel_size=kernel_size1, activation='relu', kernel_constraint=max_norm(3))\n",
    "    conv11 = conv1(embedding_out1)\n",
    "    conv12 = conv1(embedding_out2)\n",
    "    add1 = Add()([conv11, conv12])\n",
    "    pool1 = MaxPooling1D(MAX_SEQUENCE_LENGTH - kernel_size1 + 1)(add1)\n",
    "    out1 = Flatten()(pool1)\n",
    "\n",
    "    kernel_size2 = 4\n",
    "    conv2 = Conv1D(filters=100, kernel_size=kernel_size2, activation='relu', kernel_constraint=max_norm(3))\n",
    "    conv21 = conv2(embedding_out1)\n",
    "    conv22 = conv2(embedding_out2)\n",
    "    add2 = Add()([conv21, conv22])\n",
    "    pool2 = MaxPooling1D(MAX_SEQUENCE_LENGTH - kernel_size1 + 1)(add2)\n",
    "    out2 = Flatten()(pool2)\n",
    "\n",
    "    kernel_size3 = 5\n",
    "    conv3 = Conv1D(filters=100, kernel_size=kernel_size3, activation='relu', kernel_constraint=max_norm(3))\n",
    "    conv31 = conv3(embedding_out1)\n",
    "    conv32 = conv3(embedding_out2)\n",
    "    add3 = Add()([conv31, conv32])\n",
    "    pool3 = MaxPooling1D(MAX_SEQUENCE_LENGTH - kernel_size1 + 1)(add3)\n",
    "    out3 = Flatten()(pool3)\n",
    "\n",
    "    final_out = Concatenate()([out1, out2, out3])\n",
    "\n",
    "    final_out = Dropout(0.5)(final_out)\n",
    "\n",
    "    final_out = Dense(1, activation='sigmoid')(final_out)\n",
    "\n",
    "    # Pack it all up into a model\n",
    "    model = Model(inputs=input_vec, outputs=final_out)\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpKJEVdHGNm3"
   },
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9TdrVtEGPwX"
   },
   "outputs": [],
   "source": [
    "multi_channel_model = train_multichannel_model(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1YyeFxKOGj2-"
   },
   "outputs": [],
   "source": [
    "multi_channel_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbxHAWgiGQlG"
   },
   "source": [
    "### Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TOo-iAjNGpnd"
   },
   "outputs": [],
   "source": [
    "multi_channel_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gI8u1qg6HKRS"
   },
   "source": [
    "### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4B8sLcruHMGw"
   },
   "outputs": [],
   "source": [
    "# Create a checkpoint\n",
    "filepath = MODEL_DIR + 'multi_channel_model_best.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnn5UJKEHVlm"
   },
   "outputs": [],
   "source": [
    "multi_channel_model_trained = multi_channel_model.fit(x=train_indices, y=train_labels,\n",
    "                                  batch_size=batch_size, nb_epoch=num_epochs, callbacks=callbacks_list, verbose=2,\n",
    "                                  validation_data=(val_indices, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQoL5qaLHcj_"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yg6bs-twKiSO"
   },
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "multi_channel_model_json = multi_channel_model.to_json()\n",
    "\n",
    "with open(MODEL_DIR + 'multi_channel_model_json.json', 'w') as json_file:\n",
    "    json_file.write(multi_channel_model)\n",
    "\n",
    "print('Saved Multi Channel model architecture to the disk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjcr6jLkKzk4"
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NyHTAmMK5Q_"
   },
   "outputs": [],
   "source": [
    "# Load architecture\n",
    "with open(MODEL_DIR + 'multi_channel_model_json.json', 'r') as json_file:\n",
    "    model_json = json_file.read()\n",
    "\n",
    "multi_channel_model = model_from_json(model_json)\n",
    "\n",
    "# Load weights\n",
    "multi_channel_model.load_weights(MODEL_DIR + 'multi_channel_model_best.hdf5')\n",
    "print('Loaded Non Static model from the disk')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "kim_cnn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
